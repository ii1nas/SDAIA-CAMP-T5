{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN3a2lC+sa1JCszAxbQktck"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["---\n","---\n","# Convolutional Neural Network\n","---\n","---"],"metadata":{"id":"-2_OohltEMFm"}},{"cell_type":"markdown","source":["In this notebook, we will explore and compare different architectural approaches for image processing tasks, specifically focusing on comparing traditional architectures and Convolutional Neural Networks (CNNs). We'll work with multiple image datasets to gain hands-on experience and see how these models perform in practical scenarios."],"metadata":{"id":"O3uwkQ9UF5o8"}},{"cell_type":"markdown","source":["## Libraries Imports"],"metadata":{"id":"XQPsWNhYF50E"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"YMaoWoduIipd"},"outputs":[],"source":["#import tensorflow and keras\n","import tensorflow as tf\n","from keras import layers, models, datasets\n","\n","#import pandas and matplotlib for accurcy visualization\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","source":["## Loading Data"],"metadata":{"id":"N0AcTxAUG8eR"}},{"cell_type":"markdown","source":["### Fashion MNIST\n","It dataset is a modern alternative to the traditional MNIST dataset, a staple in the machine learning community for handwriting recognition tasks. Developed as a more challenging and representative benchmark for machine learning algorithms, Fashion MNIST comprises grayscale images of various fashion products from 10 different categories.\n","\n","### Key Features of Fashion MNIST:\n","\n","- **Dataset Size:** Fashion MNIST consists of a total of 70,000 images, divided into a training set of 60,000 images and a test set of 10,000 images.\n","- **Image Details:** Each image in the dataset is 28x28 pixels in size, represented in grayscale (1 channel).\n","- **Categories:** The dataset includes 10 types of fashion items:\n",">- T-shirt/top\n",">- Trouser\n",">- Pullover\n",">- Dress\n",">- Coat\n",">- Sandal\n",">- Shirt\n",">- Sneaker\n",">- Bag\n",">- Ankle boot\n","\n","### Purpose and Applications:\n","\n","Fashion MNIST is specifically designed to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits but involves a different, arguably more complex, set of images.\n","\n"],"metadata":{"id":"eQjkVkKEHjIN"}},{"cell_type":"code","source":["# Loading the Dataset from Keras\n","fashion_mnist = datasets.fashion_mnist\n","(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n","\n","#Spliting the training Dataset into train and validation\n","X_valid, X_train = X_train_full[:4000] / 255.0, X_train_full[4000:] / 255.0\n","y_valid, y_train = y_train_full[:4000], y_train_full[4000:]"],"metadata":{"id":"GuUJ4ockHEcy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Modeling"],"metadata":{"id":"VAUz-l7kIA6G"}},{"cell_type":"markdown","source":["Now Let's try the following traditional approach for deep neural net"],"metadata":{"id":"E3nRpYiLJyJV"}},{"cell_type":"code","source":["model = models.Sequential([\n","                          layers.Flatten(input_shape=[28, 28]),\n","                          layers.Dense(16, activation=\"relu\"),\n","                          layers.Dense(8, activation=\"relu\"),\n","                          layers.Dense(10, activation=\"softmax\")\n","                          ])\n","\n","# Compile the model\n","model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nW0dwRmeIB8V","executionInfo":{"status":"ok","timestamp":1714149548057,"user_tz":-180,"elapsed":13,"user":{"displayName":"Mohamed Eldeeb","userId":"14305021883410993794"}},"outputId":"d9e3c721-3c60-459d-c1c9-d4bbae09f6e1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_5\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," flatten_5 (Flatten)         (None, 784)               0         \n","                                                                 \n"," dense_15 (Dense)            (None, 16)                12560     \n","                                                                 \n"," dense_16 (Dense)            (None, 8)                 136       \n","                                                                 \n"," dense_17 (Dense)            (None, 10)                90        \n","                                                                 \n","=================================================================\n","Total params: 12786 (49.95 KB)\n","Trainable params: 12786 (49.95 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["#Fit or Train the model\n","history = model.fit(X_train, y_train, batch_size=1000, epochs=10, validation_data=(X_valid, y_valid))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qh_PeFP9IQFd","executionInfo":{"status":"ok","timestamp":1714149555497,"user_tz":-180,"elapsed":7445,"user":{"displayName":"Mohamed Eldeeb","userId":"14305021883410993794"}},"outputId":"044d498d-e229-4bbf-899c-64711fbde540"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","56/56 [==============================] - 2s 10ms/step - loss: 1.6708 - accuracy: 0.4387 - val_loss: 1.1910 - val_accuracy: 0.6403\n","Epoch 2/10\n","56/56 [==============================] - 0s 6ms/step - loss: 0.9932 - accuracy: 0.6900 - val_loss: 0.8546 - val_accuracy: 0.7450\n","Epoch 3/10\n","56/56 [==============================] - 0s 6ms/step - loss: 0.7537 - accuracy: 0.7688 - val_loss: 0.6946 - val_accuracy: 0.7920\n","Epoch 4/10\n","56/56 [==============================] - 0s 6ms/step - loss: 0.6289 - accuracy: 0.8002 - val_loss: 0.6085 - val_accuracy: 0.8067\n","Epoch 5/10\n","56/56 [==============================] - 0s 7ms/step - loss: 0.5674 - accuracy: 0.8153 - val_loss: 0.5671 - val_accuracy: 0.8175\n","Epoch 6/10\n","56/56 [==============================] - 0s 6ms/step - loss: 0.5323 - accuracy: 0.8242 - val_loss: 0.5374 - val_accuracy: 0.8282\n","Epoch 7/10\n","56/56 [==============================] - 0s 8ms/step - loss: 0.5096 - accuracy: 0.8299 - val_loss: 0.5179 - val_accuracy: 0.8288\n","Epoch 8/10\n","56/56 [==============================] - 0s 8ms/step - loss: 0.4912 - accuracy: 0.8352 - val_loss: 0.5065 - val_accuracy: 0.8325\n","Epoch 9/10\n","56/56 [==============================] - 0s 7ms/step - loss: 0.4768 - accuracy: 0.8386 - val_loss: 0.4886 - val_accuracy: 0.8407\n","Epoch 10/10\n","56/56 [==============================] - 0s 9ms/step - loss: 0.4684 - accuracy: 0.8409 - val_loss: 0.4887 - val_accuracy: 0.8357\n"]}]},{"cell_type":"markdown","metadata":{"id":"huzDQx7EEHID"},"source":["## Model evaluating\n","Once you have attained satisfactory validation accuracy for your model, it is crucial to evaluate its performance on the test set to estimate the generalization error before deploying the model to production. This can be accomplished conveniently using the evaluate() method. This method supports various arguments, including batch_size or sample_weight. For further details, please refer to the documentation."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1511,"status":"ok","timestamp":1714149557004,"user":{"displayName":"Mohamed Eldeeb","userId":"14305021883410993794"},"user_tz":-180},"id":"sutK4SpVClVp","outputId":"5fec8398-823c-4ace-f196-7887010cf483"},"outputs":[{"output_type":"stream","name":"stdout","text":["313/313 [==============================] - 1s 2ms/step - loss: 68.9942 - accuracy: 0.8153\n"]},{"output_type":"execute_result","data":{"text/plain":["[68.99422454833984, 0.8152999877929688]"]},"metadata":{},"execution_count":30}],"source":["#Model evaluating\n","model.evaluate(X_test, y_test)"]},{"cell_type":"markdown","source":["**It looks relatively good but let's try another more advanced approach**\n","\n"],"metadata":{"id":"wKnIsxUnKbgS"}},{"cell_type":"markdown","source":["## Introducing Convolutional Neural Networks (CNNs)"],"metadata":{"id":"OpRsP2BELDsv"}},{"cell_type":"markdown","source":["Convolutional Neural Networks (CNNs) are a class of deep neural networks that are especially effective for analyzing visual imagery. They have become a cornerstone technology in the field of computer vision, particularly excelling in tasks like image and video recognition, image classification, and also applications such as medical image analysis and autonomous driving.\n","![CNN](https://saturncloud.io/images/blog/a-cnn-sequence-to-classify-handwritten-digits.webp)\n","### Key Components of CNNs:\n","\n","1. **Convolutional Layers:**\n","   - These are the core building blocks of a CNN. The convolutional layer applies a number of filters to the input. Each filter detects different features such as edges, colors, or more complex shapes. The output of this layer is called a feature map, which highlights the areas of the input image most activated by the filter.\n","\n","2. **ReLU Layer (Activation):**\n","   - After each convolution operation, an activation function such as the Rectified Linear Unit (ReLU) is applied to introduce non-linearity into the model. Non-linearity is crucial as it helps the network learn more complex patterns in the data.\n","\n","3. **Pooling Layers:**\n","   - Pooling (also known as subsampling or downsampling) reduces the dimensionality of each feature map but retains the most important information. Max pooling is a common technique used to reduce the spatial dimensions of the input volume for the next convolution layer. It works by selecting the maximum value from each cluster of neurons at the prior layer.\n","\n","4. **Fully Connected Layers:**\n","   - After several convolutional and pooling layers, the high-level reasoning in the neural network is done via fully connected layers. Neurons in a fully connected layer have full connections to all activations in the previous layer. This part of the network is typically responsible for assembling the features extracted by the convolutional layers and pooling layers to form the final outputs.\n","\n","5. **Output Layer:**\n","   - The final layer uses an activation function such as softmax (for classification tasks) to map the output of the last fully connected layer to probability distributions over classes."],"metadata":{"id":"oKMmO-kyLPNL"}},{"cell_type":"code","source":["model = models.Sequential([\n","                          layers.Conv2D(32, kernel_size=(4, 4), activation='relu', input_shape=(28, 28,1)),\n","                          layers.MaxPool2D(2,2),\n","                          layers.Flatten(),\n","                          layers.Dense(16, activation=\"relu\"),\n","                          layers.Dense(8, activation=\"relu\"),\n","                          layers.Dense(10, activation=\"softmax\")\n","                          ])\n","\n","# Compile the model\n","model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yQvxiJnSLC19","executionInfo":{"status":"ok","timestamp":1714149557836,"user_tz":-180,"elapsed":838,"user":{"displayName":"Mohamed Eldeeb","userId":"14305021883410993794"}},"outputId":"ae6a2176-192a-4b6f-f37b-4a3941a514f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_6\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_3 (Conv2D)           (None, 25, 25, 32)        544       \n","                                                                 \n"," max_pooling2d_3 (MaxPoolin  (None, 12, 12, 32)        0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_6 (Flatten)         (None, 4608)              0         \n","                                                                 \n"," dense_18 (Dense)            (None, 16)                73744     \n","                                                                 \n"," dense_19 (Dense)            (None, 8)                 136       \n","                                                                 \n"," dense_20 (Dense)            (None, 10)                90        \n","                                                                 \n","=================================================================\n","Total params: 74514 (291.07 KB)\n","Trainable params: 74514 (291.07 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["### How CNNs Process an Image:\n","![CNN process](https://i.stack.imgur.com/bN2iA.png)\n","- **Input:** The process begins with an input image, which is passed through a series of convolutional, nonlinear, and pooling layers.\n","- **Feature Learning:** Through the convolutional layers, the network learns to identify various features of the image. Early layers might detect simple features like edges and textures, while deeper layers can identify more complex features like patterns or objects.\n","- **Classification:** After feature extraction, the network uses fully connected layers to determine the content of the image based on the presence of learned features, culminating in a classification decision."],"metadata":{"id":"7a2spEOqMXXj"}},{"cell_type":"code","source":["# Compile the model\n","model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(X_train, y_train, batch_size=2000, epochs=10, validation_data=(X_valid, y_valid))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SFwOv1X_KXrS","outputId":"bdfec86b-4391-4e33-b3ed-a10365fa82df","executionInfo":{"status":"ok","timestamp":1714149820592,"user_tz":-180,"elapsed":262762,"user":{"displayName":"Mohamed Eldeeb","userId":"14305021883410993794"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","28/28 [==============================] - 23s 789ms/step - loss: 1.7523 - accuracy: 0.3640 - val_loss: 1.2654 - val_accuracy: 0.5650\n","Epoch 2/10\n","28/28 [==============================] - 22s 789ms/step - loss: 1.1041 - accuracy: 0.6103 - val_loss: 0.9593 - val_accuracy: 0.6888\n","Epoch 3/10\n","28/28 [==============================] - 22s 799ms/step - loss: 0.9014 - accuracy: 0.7032 - val_loss: 0.8314 - val_accuracy: 0.7178\n","Epoch 4/10\n","28/28 [==============================] - 23s 844ms/step - loss: 0.8003 - accuracy: 0.7280 - val_loss: 0.7598 - val_accuracy: 0.7352\n","Epoch 5/10\n","28/28 [==============================] - 22s 781ms/step - loss: 0.7396 - accuracy: 0.7442 - val_loss: 0.7074 - val_accuracy: 0.7498\n","Epoch 6/10\n","28/28 [==============================] - 23s 818ms/step - loss: 0.6866 - accuracy: 0.7454 - val_loss: 0.6533 - val_accuracy: 0.7500\n","Epoch 7/10\n","28/28 [==============================] - 23s 820ms/step - loss: 0.6048 - accuracy: 0.7943 - val_loss: 0.5665 - val_accuracy: 0.8235\n","Epoch 8/10\n","28/28 [==============================] - 23s 809ms/step - loss: 0.5230 - accuracy: 0.8324 - val_loss: 0.5177 - val_accuracy: 0.8303\n","Epoch 9/10\n","28/28 [==============================] - 21s 766ms/step - loss: 0.4742 - accuracy: 0.8397 - val_loss: 0.4758 - val_accuracy: 0.8422\n","Epoch 10/10\n","28/28 [==============================] - 24s 852ms/step - loss: 0.4491 - accuracy: 0.8477 - val_loss: 0.4661 - val_accuracy: 0.8435\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7bada1a4fee0>"]},"metadata":{},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"fl45OusUPE0U"},"source":["## Model evaluating\n","Once you have attained satisfactory validation accuracy for your model, it is crucial to evaluate its performance on the test set to estimate the generalization error before deploying the model to production. This can be accomplished conveniently using the evaluate() method. This method supports various arguments, including batch_size or sample_weight. For further details, please refer to the documentation."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1375,"status":"ok","timestamp":1714149821964,"user":{"displayName":"Mohamed Eldeeb","userId":"14305021883410993794"},"user_tz":-180},"outputId":"e69392b7-e383-45e7-8bba-04dc8fb7304c","id":"S6q2rOAXPE0U"},"outputs":[{"output_type":"stream","name":"stdout","text":["313/313 [==============================] - 1s 4ms/step - loss: 58.2136 - accuracy: 0.8378\n"]},{"output_type":"execute_result","data":{"text/plain":["[58.213592529296875, 0.8378000259399414]"]},"metadata":{},"execution_count":33}],"source":["#Model evaluating\n","model.evaluate(X_test, y_test)"]},{"cell_type":"markdown","source":["### Advantages of CNNs:\n","\n","- **Automatic Feature Extraction:** Unlike traditional algorithms, CNNs learn to detect features without needing any explicit programming, making them highly effective for tasks involving complex visual inputs.\n","- **Spatial Hierarchies:** CNNs can learn spatial hierarchies of features thanks to their deep architecture. They can recognize objects regardless of variations in their appearance or in different environments.\n","- **Efficiency:** Once trained, CNNs can make predictions rapidly, making them suitable for applications requiring real-time processing.\n","\n","CNNs are foundational to modern image recognition systems and continue to push the boundaries in fields reliant on image understanding."],"metadata":{"id":"nGUJEfyIMh4y"}},{"cell_type":"markdown","source":["# Now let's try a more challenging problem"],"metadata":{"id":"e9OLZn37gv7b"}},{"cell_type":"markdown","source":["## Loading Data"],"metadata":{"id":"vVY0vMoqMiCc"}},{"cell_type":"markdown","source":["The above CNN method truly shines when used with big images and especially multi-channel images"],"metadata":{"id":"lyDK-dh3QqBX"}},{"cell_type":"markdown","source":["**The CIFAR-100** dataset is a well-known dataset used in machine learning and computer vision for evaluating image recognition algorithms. It is a more complex and diverse dataset compared to its counterpart, CIFAR-10, primarily due to the larger number of classes.\n","\n","![Cifar100](https://datasets.activeloop.ai/wp-content/uploads/2022/09/CIFAR-100-dataset-Activeloop-Platform-visualization-image.webp)\n","\n",">- CIFAR-100 contains 60,000 32x32 color images.\n",">- The images are divided into 100 classes, each containing 600 images.\n",">- The 100 classes are grouped into 20 superclasses.\n",">- Each superclass encompasses several classes that are more specific; for example, the \"aquatic mammals\" superclass includes classes like \"beaver\", \"dolphin\", and \"otter\"."],"metadata":{"id":"evYaGKSWU0vA"}},{"cell_type":"code","source":["# Loading the Dataset from Keras\n","cifar = datasets.cifar100\n","(X_train_full, y_train_full), (X_test, y_test) = cifar.load_data(label_mode='coarse')\n","\n","#Spliting the training Dataset into train and validation\n","X_valid, X_train = X_train_full[:4000] / 255.0, X_train_full[4000:] / 255.0\n","y_valid, y_train = y_train_full[:4000], y_train_full[4000:]"],"metadata":{"id":"U9-GLxo2Qjd5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train_full.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PrJscjleXNlH","executionInfo":{"status":"ok","timestamp":1714151327443,"user_tz":-180,"elapsed":3,"user":{"displayName":"Mohamed Eldeeb","userId":"14305021883410993794"}},"outputId":"cb4bacec-4706-4120-e1ab-a3d079a0e8cc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(50000, 32, 32, 3)"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":["np.unique(y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vJhIM7vxXcb9","executionInfo":{"status":"ok","timestamp":1714151327780,"user_tz":-180,"elapsed":4,"user":{"displayName":"Mohamed Eldeeb","userId":"14305021883410993794"}},"outputId":"aa88ab7b-f364-40ef-9ff7-b023cc7c9dcf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n","       17, 18, 19])"]},"metadata":{},"execution_count":52}]},{"cell_type":"markdown","source":["## Modeling"],"metadata":{"id":"biroYNwNXLAd"}},{"cell_type":"code","source":["model = models.Sequential([\n","                          layers.Flatten(input_shape=[32, 32, 3]),\n","                          layers.Dense(16, activation=\"relu\"),\n","                          layers.Dense(8, activation=\"relu\"),\n","                          layers.Dense(20, activation=\"softmax\")\n","                          ])\n","\n","# Compile the model\n","model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714151333737,"user_tz":-180,"elapsed":13,"user":{"displayName":"Mohamed Eldeeb","userId":"14305021883410993794"}},"outputId":"ffe91415-f8f2-4b09-e383-4793c03c87dc","id":"bJeZEynHXHuy"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_8\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," flatten_8 (Flatten)         (None, 3072)              0         \n","                                                                 \n"," dense_24 (Dense)            (None, 16)                49168     \n","                                                                 \n"," dense_25 (Dense)            (None, 8)                 136       \n","                                                                 \n"," dense_26 (Dense)            (None, 20)                180       \n","                                                                 \n","=================================================================\n","Total params: 49484 (193.30 KB)\n","Trainable params: 49484 (193.30 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["#Fit or Train the model\n","history = model.fit(X_train, y_train, batch_size=1000, epochs=10, validation_data=(X_valid, y_valid))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714151349887,"user_tz":-180,"elapsed":14947,"user":{"displayName":"Mohamed Eldeeb","userId":"14305021883410993794"}},"outputId":"a4e64528-b04b-48aa-aee3-44effb76c821","id":"hgIS4VEtXHu6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","46/46 [==============================] - 2s 27ms/step - loss: 2.9865 - accuracy: 0.0547 - val_loss: 2.9645 - val_accuracy: 0.0665\n","Epoch 2/10\n","46/46 [==============================] - 1s 24ms/step - loss: 2.9553 - accuracy: 0.0694 - val_loss: 2.9400 - val_accuracy: 0.0822\n","Epoch 3/10\n","46/46 [==============================] - 1s 30ms/step - loss: 2.9325 - accuracy: 0.0804 - val_loss: 2.9207 - val_accuracy: 0.0795\n","Epoch 4/10\n","46/46 [==============================] - 2s 37ms/step - loss: 2.9120 - accuracy: 0.0812 - val_loss: 2.8962 - val_accuracy: 0.0828\n","Epoch 5/10\n","46/46 [==============================] - 1s 22ms/step - loss: 2.8921 - accuracy: 0.0826 - val_loss: 2.8793 - val_accuracy: 0.0850\n","Epoch 6/10\n","46/46 [==============================] - 1s 21ms/step - loss: 2.8764 - accuracy: 0.0827 - val_loss: 2.8691 - val_accuracy: 0.0843\n","Epoch 7/10\n","46/46 [==============================] - 1s 21ms/step - loss: 2.8616 - accuracy: 0.0836 - val_loss: 2.8656 - val_accuracy: 0.0822\n","Epoch 8/10\n","46/46 [==============================] - 1s 21ms/step - loss: 2.8499 - accuracy: 0.0875 - val_loss: 2.8450 - val_accuracy: 0.0845\n","Epoch 9/10\n","46/46 [==============================] - 1s 21ms/step - loss: 2.8384 - accuracy: 0.0942 - val_loss: 2.8271 - val_accuracy: 0.1000\n","Epoch 10/10\n","46/46 [==============================] - 1s 21ms/step - loss: 2.8137 - accuracy: 0.1078 - val_loss: 2.8013 - val_accuracy: 0.1060\n"]}]},{"cell_type":"markdown","metadata":{"id":"0I_ZZf4EXHu7"},"source":["## Model evaluating"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2458,"status":"ok","timestamp":1714154227713,"user":{"displayName":"Mohamed Eldeeb","userId":"14305021883410993794"},"user_tz":-180},"outputId":"1fa1d901-941e-4ac0-9c8f-b9031538ccda","id":"BNueIWyVXHu7"},"outputs":[{"output_type":"stream","name":"stdout","text":["313/313 [==============================] - 2s 4ms/step - loss: 100.5143 - accuracy: 0.1051\n"]},{"output_type":"execute_result","data":{"text/plain":["[100.51434326171875, 0.10509999841451645]"]},"metadata":{},"execution_count":55}],"source":["#Model evaluating\n","model.evaluate(X_test, y_test)"]},{"cell_type":"markdown","source":["As observed the traditional way does not work well with complex images. Now let's try out the CNN approach"],"metadata":{"id":"j2JoehUujeR0"}},{"cell_type":"code","source":["model = models.Sequential([\n","                          layers.Conv2D(32, kernel_size=(4, 4), activation='relu', input_shape=(32, 32,3)),\n","                          layers.MaxPool2D(2,2),\n","                          layers.Flatten(),\n","                          layers.Dense(16, activation=\"relu\"),\n","                          layers.Dense(8, activation=\"relu\"),\n","                          layers.Dense(20, activation=\"softmax\")\n","                          ])\n","\n","# Compile the model\n","model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714154529519,"user_tz":-180,"elapsed":543,"user":{"displayName":"Mohamed Eldeeb","userId":"14305021883410993794"}},"outputId":"70176dd9-e9b3-4d0c-a8a4-da35cbcad3f9","id":"2fQKNUHQkbS8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_10\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_5 (Conv2D)           (None, 29, 29, 32)        1568      \n","                                                                 \n"," max_pooling2d_5 (MaxPoolin  (None, 14, 14, 32)        0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_10 (Flatten)        (None, 6272)              0         \n","                                                                 \n"," dense_30 (Dense)            (None, 16)                100368    \n","                                                                 \n"," dense_31 (Dense)            (None, 8)                 136       \n","                                                                 \n"," dense_32 (Dense)            (None, 20)                180       \n","                                                                 \n","=================================================================\n","Total params: 102252 (399.42 KB)\n","Trainable params: 102252 (399.42 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# Compile the model\n","model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(X_train, y_train, batch_size=2000, epochs=10, validation_data=(X_valid, y_valid))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d23a2980-6084-4414-ac30-e7db84486f22","executionInfo":{"status":"ok","timestamp":1714154856417,"user_tz":-180,"elapsed":324820,"user":{"displayName":"Mohamed Eldeeb","userId":"14305021883410993794"}},"id":"-pnGn9cWkbTT"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","23/23 [==============================] - 32s 1s/step - loss: 2.9509 - accuracy: 0.0936 - val_loss: 2.9130 - val_accuracy: 0.1060\n","Epoch 2/10\n","23/23 [==============================] - 31s 1s/step - loss: 2.8797 - accuracy: 0.1191 - val_loss: 2.8528 - val_accuracy: 0.1230\n","Epoch 3/10\n","23/23 [==============================] - 32s 1s/step - loss: 2.8213 - accuracy: 0.1310 - val_loss: 2.7889 - val_accuracy: 0.1305\n","Epoch 4/10\n","23/23 [==============================] - 29s 1s/step - loss: 2.7517 - accuracy: 0.1451 - val_loss: 2.7171 - val_accuracy: 0.1353\n","Epoch 5/10\n","23/23 [==============================] - 30s 1s/step - loss: 2.6778 - accuracy: 0.1574 - val_loss: 2.6454 - val_accuracy: 0.1517\n","Epoch 6/10\n","23/23 [==============================] - 28s 1s/step - loss: 2.6152 - accuracy: 0.1789 - val_loss: 2.5873 - val_accuracy: 0.1795\n","Epoch 7/10\n","23/23 [==============================] - 29s 1s/step - loss: 2.5614 - accuracy: 0.1966 - val_loss: 2.5593 - val_accuracy: 0.1930\n","Epoch 8/10\n","23/23 [==============================] - 29s 1s/step - loss: 2.5264 - accuracy: 0.2166 - val_loss: 2.5108 - val_accuracy: 0.2135\n","Epoch 9/10\n","23/23 [==============================] - 31s 1s/step - loss: 2.4889 - accuracy: 0.2277 - val_loss: 2.4848 - val_accuracy: 0.2230\n","Epoch 10/10\n","23/23 [==============================] - 30s 1s/step - loss: 2.4607 - accuracy: 0.2353 - val_loss: 2.4762 - val_accuracy: 0.2303\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7bada0368a30>"]},"metadata":{},"execution_count":59}]},{"cell_type":"code","source":["#Model evaluating\n","model.evaluate(X_test, y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nPk1T6DNmHhy","executionInfo":{"status":"ok","timestamp":1714154933760,"user_tz":-180,"elapsed":3379,"user":{"displayName":"Mohamed Eldeeb","userId":"14305021883410993794"}},"outputId":"976e3707-145a-4037-b2d3-d58968fb268c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["313/313 [==============================] - 3s 7ms/step - loss: 280.3564 - accuracy: 0.1821\n"]},{"output_type":"execute_result","data":{"text/plain":["[280.3564453125, 0.18209999799728394]"]},"metadata":{},"execution_count":60}]},{"cell_type":"markdown","source":["As you can see the accuracy here improved upon using the new CNN approach.\n","\n","It's also worth noting that this is a simple CNN Design in real life the neural nets are much more complex and performs much better."],"metadata":{"id":"9OsmixiGmDgq"}}]}